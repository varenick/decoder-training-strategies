{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "\n",
    "with open('text8') as f:\n",
    "    for line in f:\n",
    "        text.append(line)\n",
    "    text = ''.join(text)\n",
    "\n",
    "vocab_size = len(set(text)) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " anarchism\n",
      "[ 1  2 15  2 19  4  9 10 20 14]\n"
     ]
    }
   ],
   "source": [
    "EOS_id = 0\n",
    "\n",
    "def char_to_id(char):\n",
    "    if char == ' ':\n",
    "        return 1\n",
    "    else:\n",
    "        return ord(char) - ord('a') + 2\n",
    "\n",
    "def id_to_char(i):\n",
    "    if i == 0:\n",
    "        return ''\n",
    "    elif i == 1:\n",
    "        return ' '\n",
    "    else:\n",
    "        return chr(ord('a') + i - 2)\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "text_ids = {'full': np.array(list(map(char_to_id, text)))}\n",
    "print(text[:10])\n",
    "print(text_ids['full'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000000\n"
     ]
    }
   ],
   "source": [
    "text_length = len(text)\n",
    "print(text_length)\n",
    "\n",
    "text_ids['train'] = text_ids['full'][:int(text_length * 0.8)]\n",
    "text_ids['eval'] = text_ids['full'][int(text_length * 0.8):int(text_length * 0.9)]\n",
    "text_ids['decode'] = text_ids['full'][int(text_length * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(text, chunk_length, batch_size):\n",
    "    while True:\n",
    "        chunk_starts = np.random.randint(len(text) - chunk_length, size=batch_size)\n",
    "        yield np.array([text[chunk_start:chunk_start + chunk_length] for chunk_start in chunk_starts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  5  6  4 13 10 15  6  5  1]\n",
      " [ 2 19 12  6 21  1 24 10 21  9]]\n",
      "[[ 1  5  6 15 10 13  4  6  5  1]\n",
      " [21  6 12 19  2  1  9 21 10 24]]\n"
     ]
    }
   ],
   "source": [
    "def revert_words(chunk_batch):\n",
    "    rev_chunk_batch = []\n",
    "    for chunk in chunk_batch:\n",
    "        words = np.split(chunk, np.flatnonzero(chunk == char_to_id(' ')))\n",
    "        rev_chunk = np.hstack([words[0][::-1]] + [np.roll(word[::-1], 1) for word in words[1:]])\n",
    "        rev_chunk_batch.append(rev_chunk)\n",
    "    return np.array(rev_chunk_batch)\n",
    "\n",
    "chunk_batch = next(get_batch(text_ids['decode'], 10, 2))\n",
    "print(chunk_batch)\n",
    "print(revert_words(chunk_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses_av_mean = {}\n",
    "train_accs_av_mean = {}\n",
    "\n",
    "eval_losses_av_mean = {}\n",
    "eval_accs_av_mean = {}\n",
    "\n",
    "train_losses_av_std = {}\n",
    "train_accs_av_std = {}\n",
    "\n",
    "eval_losses_av_std = {}\n",
    "eval_accs_av_std = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_hidden, num_layers=1, dropout_rate=0.2):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_hidden = num_hidden\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.enc_rnn = nn.LSTM(embed_dim, num_hidden, num_layers,\n",
    "                               batch_first=True, bidirectional=True)\n",
    "        self.dec_cell = nn.LSTMCell(embed_dim, num_hidden * num_layers * 2)\n",
    "        self.output_proj = nn.Linear(num_hidden * num_layers * 2, vocab_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "    def forward(self, input_chunk, target_chunk=None, feed_mode='same', output_mode='argmax'):\n",
    "        need_target_chunk = (feed_mode == 'teacher-forcing')\n",
    "        if target_chunk is None and need_target_chunk:\n",
    "            raise ValueError(\"You should provide target_chunk when using feed_mode '{}'\".format(feed_mode))\n",
    "        if feed_mode not in ['same', 'teacher-forcing', 'argmax', 'sampling']:\n",
    "            raise ValueError(\"Invalid feed_mode: '{}'\".format(feed_mode))\n",
    "        if output_mode not in ['argmax', 'sampling']:\n",
    "            raise ValueError(\"Invalid output_mode: '{}'\".format(output_mode))\n",
    "            \n",
    "        batch_size, chunk_length = input_chunk.size()\n",
    "        \n",
    "        # Encoder:\n",
    "        input_chunk_emb = self.embedding(input_chunk)\n",
    "        enc_h_first = autograd.Variable(\n",
    "            torch.zeros(self.num_layers * 2, batch_size, self.num_hidden).cuda(),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        enc_c_first = autograd.Variable(\n",
    "            torch.zeros(self.num_layers * 2, batch_size, self.num_hidden).cuda(),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        _, enc_hc_last = self.enc_rnn(input_chunk_emb, (enc_h_first, enc_c_first))\n",
    "        \n",
    "        # Decoder:\n",
    "        dec_h = torch.transpose(enc_hc_last[0], 0, 1).contiguous().view(batch_size, -1)\n",
    "        dec_c = torch.transpose(enc_hc_last[1], 0, 1).contiguous().view(batch_size, -1)\n",
    "        dec_feed = None\n",
    "        dec_feed_emb = autograd.Variable(\n",
    "            torch.zeros(batch_size, self.embed_dim).cuda(),\n",
    "            requires_grad=False\n",
    "        )\n",
    "        dec_unscaled_logits = []\n",
    "        dec_outputs = []\n",
    "        self.dec_feeds = []\n",
    "        \n",
    "        target_chunk_emb = None\n",
    "        if need_target_chunk:\n",
    "            target_chunk_emb = self.embedding(target_chunk)\n",
    "            \n",
    "        for t in range(chunk_length):\n",
    "            dec_h, dec_c = self.dec_cell(self.dropout(dec_feed_emb), (dec_h, dec_c))\n",
    "            dec_unscaled_logits.append(self.output_proj(dec_h))\n",
    "            \n",
    "            if output_mode == 'argmax':\n",
    "                dec_outputs.append(torch.max(dec_unscaled_logits[-1], dim=1)[1])\n",
    "            elif output_mode == 'sampling':\n",
    "                dec_outputs.append(torch.multinomial(torch.exp(dec_unscaled_logits[-1]), 1).view(batch_size))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid output_mode: '{}'\".format(output_mode))\n",
    "                \n",
    "            if feed_mode == 'same':\n",
    "                dec_feed = dec_outputs[-1]\n",
    "                dec_feed_emb = self.embedding(dec_feed.view(batch_size, 1)).view(batch_size, self.embed_dim)\n",
    "            elif feed_mode == 'teacher-forcing':\n",
    "                dec_feed = target_chunk[:, t]\n",
    "                dec_feed_emb = target_chunk_emb[:, t]\n",
    "            elif feed_mode == 'argmax':\n",
    "                dec_feed = torch.max(dec_unscaled_logits[-1], dim=1)[1]\n",
    "                dec_feed_emb = self.embedding(dec_feed.view(batch_size, 1)).view(batch_size, self.embed_dim)\n",
    "            elif feed_mode == 'sampling':\n",
    "                dec_feed = torch.multinomial(F.softmax(dec_unscaled_logits[-1]), 1)\n",
    "                dec_feed_emb = self.embedding(dec_feed.view(batch_size, 1)).view(batch_size, self.embed_dim)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid feed_mode: '{}'\".format(feed_mode))\n",
    "            self.dec_feeds.append(dec_feed)\n",
    "        \n",
    "        return torch.stack(dec_unscaled_logits, dim=1), torch.stack(dec_outputs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 0\n",
      "\n",
      "Step 1000\n",
      "Train loss: 1.74; train accuracy: 0.46\n",
      "Eval loss: 1.74; eval accuracy: 0.46\n",
      "pancras in st pa|  vs  |sanaacc no tssrp\n",
      "rm is duchenne m|  vs  |kr ni eeiteeee m\n",
      "rian backed gove|  vs  |nair desrrb egow\n",
      "w heavily influe|  vs  |w lliaaau elufei\n",
      "old hosted many |  vs  |dlo derohs dnaa \n",
      "o and where you |  vs  |o raa enaa ruuu \n",
      "ylvia butterfiel|  vs  |ailll lleeeeeeee\n",
      "s associated wit|  vs  |s detacaassa tiw\n",
      "116.95s from last print\n",
      "\n",
      "Step 2000\n",
      "Train loss: 0.99; train accuracy: 0.68\n",
      "Eval loss: 0.99; eval accuracy: 0.68\n",
      "or example it is|  vs  |ro elpamif xi is\n",
      "une two zero one|  vs  |enu owt orez eno\n",
      "ed demographical|  vs  |de lacimeeeeoeed\n",
      "elon almost all |  vs  |nole tsello lla \n",
      "y j nis rutkis e|  vs  |y a sii srerrs e\n",
      "ore general lite|  vs  |ero laneeeg evee\n",
      "s home to many l|  vs  |s emoh ot ylna a\n",
      "ize in the uteru|  vs  |ezi ni eht eueee\n",
      "116.90s from last print\n",
      "\n",
      "Step 3000\n",
      "Train loss: 0.79; train accuracy: 0.74\n",
      "Eval loss: 0.80; eval accuracy: 0.74\n",
      "first node eleme|  vs  |tsrif edif emeee\n",
      "tie the match ma|  vs  |eit eht hcaac am\n",
      "revelation from |  vs  |noitanreer morf \n",
      " women to settle|  vs  | nemow ot elaess\n",
      "in london is the|  vs  |ni noinum si eht\n",
      "d crystal phases|  vs  |d laoeeep nssshp\n",
      "nes hokora were |  vs  |sen oredea erew \n",
      " rituals were kn|  vs  | yltuted eref nk\n",
      "117.33s from last print\n",
      "\n",
      "Step 4000\n",
      "Train loss: 0.69; train accuracy: 0.78\n",
      "Eval loss: 0.71; eval accuracy: 0.77\n",
      " if they are con|  vs  | fi seht yra noc\n",
      "ward t e hulme l|  vs  |draw a yd  lill \n",
      "ael tv movie hav|  vs  |lea yw eufem tah\n",
      "vy mir and made |  vs  |yv ril dna deam \n",
      "ine two some min|  vs  |eni owt emis sim\n",
      "carefully config|  vs  |ylluderrc giorrr\n",
      "waiting to colle|  vs  |gnihinw ot elloc\n",
      "ent in toys espe|  vs  |tne si lseh eeve\n",
      "117.06s from last print\n",
      "\n",
      "Step 5000\n",
      "Train loss: 0.64; train accuracy: 0.79\n",
      "Eval loss: 0.65; eval accuracy: 0.79\n",
      "f the treaty of |  vs  |f eht ytteet fo \n",
      "oclassical theor|  vs  |lacisilhco rotht\n",
      "on of the christ|  vs  |no fo eht tcirhc\n",
      "not in the negat|  vs  |ton ni eht tagen\n",
      "c gaming paranor|  vs  |c gnilab roterpa\n",
      "re of education |  vs  |er fo noitaruen \n",
      "ften quite large|  vs  |netf etiuu egaes\n",
      "various sports p|  vs  |suoitae stippp p\n",
      "116.15s from last print\n",
      "\n",
      "Step 6000\n",
      "Train loss: 0.58; train accuracy: 0.81\n",
      "Eval loss: 0.59; eval accuracy: 0.81\n",
      "ar crash was mad|  vs  |ra hsara sah aam\n",
      " of its neighbou|  vs  | fo sii uoieennn\n",
      "ught to be tryin|  vs  |thgu ot eb iegrs\n",
      "raint line indic|  vs  |tniar enia ciini\n",
      "odd circumstance|  vs  |deo ecnaiimussic\n",
      "en the first wom|  vs  |ne eht tsrif mom\n",
      "licism became pr|  vs  |msiccl emacee rp\n",
      " be used to make|  vs  | eb desg re  kaa\n",
      "116.29s from last print\n",
      "\n",
      "Step 7000\n",
      "Train loss: 0.57; train accuracy: 0.82\n",
      "Eval loss: 0.57; eval accuracy: 0.82\n",
      "s point out that|  vs  |s tniop tuo taht\n",
      "rd kala sanskrit|  vs  |dr alak aiigatss\n",
      "c in the form of|  vs  |c oi eht emof fo\n",
      "alone or as an i|  vs  |enola ro sa ni i\n",
      "hree in the sula|  vs  |eerh ni eht alus\n",
      " reduced by the |  vs  | decuder yb eht \n",
      "continued to plo|  vs  |deunitooc ot ola\n",
      " kary mullis dan|  vs  | yral suilll nad\n",
      "118.22s from last print\n",
      "\n",
      "Step 8000\n",
      "Train loss: 0.51; train accuracy: 0.83\n",
      "Eval loss: 0.52; eval accuracy: 0.83\n",
      " harold ii was s|  vs  | dlarpc fi shh s\n",
      "ion at the perio|  vs  |noi ta eht oitep\n",
      "ecause of bonham|  vs  |esuace fo mannob\n",
      "s unknown as to |  vs  |s nwonnoa sa ot \n",
      " n kings of astu|  vs  | f ssiik si utsa\n",
      "ame console linu|  vs  |ema elosnoc unnn\n",
      "e three zero one|  vs  |e eerht orez eno\n",
      "over art by bob |  vs  |revo tra db b   \n",
      "118.22s from last print\n",
      "\n",
      "Step 9000\n",
      "Train loss: 0.50; train accuracy: 0.84\n",
      "Eval loss: 0.51; eval accuracy: 0.83\n",
      "f the age of dis|  vs  |f eht eka no vid\n",
      "vely as the lith|  vs  |ylev sa eht htil\n",
      "sciples aryadeva|  vs  |selaics allasyaa\n",
      "f commander is o|  vs  |f rednamooc si o\n",
      "es and towns in |  vs  |se dna snoot ni \n",
      "ntirely within t|  vs  |ylerutn nittiw t\n",
      " culture kwanzaa|  vs  | erutlaa atonaww\n",
      "e united kingdom|  vs  |e detinu modnnif\n",
      "116.81s from last print\n",
      "\n",
      "Step 10000\n",
      "Train loss: 0.49; train accuracy: 0.84\n",
      "Eval loss: 0.50; eval accuracy: 0.84\n",
      "on was first con|  vs  |no saw ttkif noc\n",
      "e methods of mea|  vs  |e sdohtem fo aem\n",
      " when his foster|  vs  | nehw sih rettof\n",
      "y philosophy is |  vs  |y yhpossisop si \n",
      "osaics the museu|  vs  |sciaso eht uesum\n",
      "ws do not consid|  vs  |sw od ton diinoc\n",
      "ht and of the sc|  vs  |th dna no eht cs\n",
      "s losing its pos|  vs  |s gniseg sti sop\n",
      "117.68s from last print\n",
      "\n",
      "Step 11000\n",
      "Train loss: 0.47; train accuracy: 0.85\n",
      "Eval loss: 0.48; eval accuracy: 0.85\n",
      "ecause except in|  vs  |esucce tcecxe ni\n",
      "ntified by a let|  vs  |deifitn yb p aal\n",
      "t schools and as|  vs  |t slotscs dna ta\n",
      "with the drug s |  vs  |htiw eht marm s \n",
      "upported governm|  vs  |detropou knrekos\n",
      "ode of austin ci|  vs  |edo fo nitsah ic\n",
      "s are situations|  vs  |s era snoitausss\n",
      "m reviews filmcr|  vs  |m sreiver rpmmif\n",
      "117.69s from last print\n",
      "\n",
      "Step 12000\n",
      "Train loss: 0.61; train accuracy: 0.81\n",
      "Eval loss: 0.62; eval accuracy: 0.80\n",
      "cted by a nation|  vs  |detc yb a noitan\n",
      "s president nixo|  vs  |s tnedimeep oots\n",
      "cond person sing|  vs  |dnoc nosrep gnis\n",
      "k of a unique po|  vs  |k fo a evunnu op\n",
      "g to be close to|  vs  |g ot eb eusla ot\n",
      "ast germany mini|  vs  |tsa ynamreg inem\n",
      "two one three of|  vs  |owt eno eerht fo\n",
      "n albert memoria|  vs  |n trebeb airomem\n",
      "116.03s from last print\n",
      "\n",
      "Step 13000\n",
      "Train loss: 0.43; train accuracy: 0.86\n",
      "Eval loss: 0.44; eval accuracy: 0.86\n",
      " light pollution|  vs  | thgih noitulsop\n",
      "one three three |  vs  |eno eerht errht \n",
      "o zero zero wome|  vs  |o orez orez emoj\n",
      "lapse of the sov|  vs  |esaal fo eht vos\n",
      "lo and toth one |  vs  |ol dna htoh eno \n",
      "her fatty materi|  vs  |reh yhwaf ieitam\n",
      "essive legislati|  vs  |evisse italslgil\n",
      "in one nine six |  vs  |ni eno enin xis \n",
      "118.25s from last print\n",
      "\n",
      "Step 14000\n",
      "Train loss: 0.45; train accuracy: 0.85\n",
      "Eval loss: 0.46; eval accuracy: 0.85\n",
      " meaning women o|  vs  | nninahm eeiaa o\n",
      " other person ma|  vs  | reppo netses am\n",
      "op have failed h|  vs  |po emah  delaf l\n",
      "five fans many r|  vs  |evif sna  eaam r\n",
      "n from the expec|  vs  |n morf eht ccexe\n",
      "ues of our day o|  vs  |seu fo rum dda o\n",
      " stadiums built |  vs  | souidaas taaub \n",
      "rella during her|  vs  |aller gnirdn reh\n",
      "117.31s from last print\n",
      "\n",
      "Step 15000\n",
      "Train loss: 0.45; train accuracy: 0.85\n",
      "Eval loss: 0.46; eval accuracy: 0.85\n",
      "electromagnetic |  vs  |cutneireeeeemle \n",
      "mmett hetfield u|  vs  |ttemm dleimeht u\n",
      "wea and her shos|  vs  |aew dna reh tohs\n",
      " the fact that w|  vs  | eht tawf taht w\n",
      "england local go|  vs  |dnalgne lacol og\n",
      "ically done at s|  vs  |yllaci enod ti s\n",
      "stages and theat|  vs  |seyats dna taaht\n",
      "unsmoke from one|  vs  |edsmslu morf eno\n",
      "117.13s from last print\n",
      "\n",
      "Step 16000\n",
      "Train loss: 0.39; train accuracy: 0.87\n",
      "Eval loss: 0.40; eval accuracy: 0.87\n",
      "the mall of amer|  vs  |eht llam fo rama\n",
      "ella in one nine|  vs  |alle ni eno enin\n",
      "e in the general|  vs  |e ni eht lareneg\n",
      "kolagen of the k|  vs  |negalob fo eht k\n",
      "elease of their |  vs  |esaele fo rieht \n",
      "nge the status q|  vs  |egn eht sutats q\n",
      "in came to feel |  vs  |ni emac ot leef \n",
      "unitarian univer|  vs  |nairatnnu revinu\n",
      "118.05s from last print\n",
      "\n",
      "Step 17000\n",
      "Train loss: 0.41; train accuracy: 0.87\n",
      "Eval loss: 0.41; eval accuracy: 0.87\n",
      "rk intellectuall|  vs  |kr llautacctttni\n",
      "urveys as detail|  vs  |syerry sa liidid\n",
      "te the formula i|  vs  |et eht alumrof i\n",
      "e united states |  vs  |e detila settts \n",
      " red top blue do|  vs  | der poo ebam od\n",
      " zero zero five |  vs  | orez orez evif \n",
      "minor changes m |  vs  |ronim segnahc m \n",
      "ary two zero two|  vs  |yra owt orez owt\n",
      "117.19s from last print\n",
      "\n",
      "Step 18000\n",
      "Train loss: 0.40; train accuracy: 0.87\n",
      "Eval loss: 0.41; eval accuracy: 0.87\n",
      " eight six one n|  vs  | thgie xis eno t\n",
      "tion name appear|  vs  |noit eman raerpp\n",
      "d cities in liby|  vs  |d seitic ni ybil\n",
      "ht for silly or |  vs  |th rof yllus ro \n",
      "the right to rem|  vs  |eht thgir ot yer\n",
      "fades changing l|  vs  |sedof gnignahc l\n",
      " with the coalit|  vs  | htiw eht tiaaoc\n",
      "he highest in th|  vs  |eh tsetiih ni ht\n",
      "117.36s from last print\n",
      "\n",
      "Step 19000\n",
      "Train loss: 0.38; train accuracy: 0.88\n",
      "Eval loss: 0.38; eval accuracy: 0.88\n",
      "mit also served |  vs  |tim wsla deeres \n",
      "volvement in a n|  vs  |tnemomcee ni a a\n",
      "e mother goddess|  vs  |e rehtom ssennog\n",
      "string solidbody|  vs  |gnirts ydoblooub\n",
      "omplex numbers i|  vs  |xelpmo srnbbid i\n",
      " illness in janu|  vs  | ssenlli ni unaj\n",
      " cairo dr octago|  vs  | oriac ri ogocco\n",
      "ch the president|  vs  |hc eht snediserp\n",
      "116.50s from last print\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20000\n",
      "Train loss: 0.47; train accuracy: 0.86\n",
      "Eval loss: 0.48; eval accuracy: 0.86\n",
      " stadium opened |  vs  | muidaes deseto \n",
      "conqueror at the|  vs  |roreziooc ta ett\n",
      " practice indist|  vs  | ecitcaep taetri\n",
      "ow b y kada off |  vs  |wo b s laob lato\n",
      "ars by a fixed r|  vs  |sra yb a tcoef a\n",
      "oung blue whales|  vs  |gnuo emab seeela\n",
      "ve its title the|  vs  |ev sai eltot eht\n",
      "t sovereignty ty|  vs  |t ytneggemees et\n",
      "115.46s from last print\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "train_batch_size = 64\n",
    "eval_batch_size = 64\n",
    "decode_batch_size = 8\n",
    "\n",
    "chunk_length = 16\n",
    "\n",
    "train_batch_gen = get_batch(text_ids['train'], batch_size=train_batch_size, chunk_length=chunk_length)\n",
    "eval_batch_gen = get_batch(text_ids['eval'], batch_size=eval_batch_size, chunk_length=chunk_length)\n",
    "\n",
    "num_runs = 2\n",
    "\n",
    "num_steps = 25000\n",
    "print_skip = 1000\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "eval_losses = []\n",
    "eval_accs = []\n",
    "\n",
    "reinforce_strategy = 'sequence'\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print('Run', run)\n",
    "    print()\n",
    "    \n",
    "    train_losses.append([])\n",
    "    train_accs.append([])\n",
    "    eval_losses.append([])\n",
    "    eval_accs.append([])\n",
    "\n",
    "    cum_train_loss = 0\n",
    "    cum_eval_loss = 0\n",
    "    cum_train_acc = 0\n",
    "    cum_eval_acc = 0\n",
    "    \n",
    "    train_av_loss = 0\n",
    "    batch_av_train_av_loss = 0\n",
    "\n",
    "    global_start_time = time()\n",
    "    last_print_time = global_start_time\n",
    "\n",
    "    model = Seq2SeqModel(vocab_size=vocab_size, embed_dim=8, num_hidden=48).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)    \n",
    "    \n",
    "    for step in range(num_steps):        \n",
    "        # Train:\n",
    "        chunk_batch = next(train_batch_gen)\n",
    "        rev_chunk_batch = revert_words(chunk_batch)\n",
    "    \n",
    "        chunk_batch_torch = autograd.Variable(torch.from_numpy(chunk_batch).cuda(), requires_grad=False)\n",
    "        rev_chunk_batch_torch = autograd.Variable(torch.from_numpy(rev_chunk_batch).cuda(), requires_grad=False)\n",
    "    \n",
    "        unscaled_logits, outputs = model(chunk_batch_torch, rev_chunk_batch_torch,\n",
    "                                         output_mode='argmax', feed_mode='sampling')\n",
    "        train_loss = loss_function(unscaled_logits.view(-1, vocab_size), rev_chunk_batch_torch.view(-1))\n",
    "        train_acc = torch.mean(torch.eq(outputs, rev_chunk_batch_torch).float())\n",
    "        \n",
    "        if reinforce_strategy == 'element':\n",
    "            rev_chunk_batch_torch_one_hot = torch.zeros(train_batch_size, chunk_length, vocab_size).cuda()\n",
    "            rev_chunk_batch_torch_one_hot.scatter_(\n",
    "                2, rev_chunk_batch_torch.data.view(train_batch_size, chunk_length, 1), 1\n",
    "            )\n",
    "            elemwise_train_loss = (-1) * F.log_softmax(\n",
    "                unscaled_logits.data.view(-1, vocab_size)\n",
    "            ).data.view(train_batch_size, chunk_length, vocab_size)[rev_chunk_batch_torch_one_hot.byte()].view(\n",
    "                train_batch_size, chunk_length\n",
    "            )\n",
    "            batch_av_train_loss = torch.mean(elemwise_train_loss, dim=0)\n",
    "            if step == 0:\n",
    "                batch_av_train_av_loss = batch_av_train_loss\n",
    "            else:\n",
    "                batch_av_train_av_loss = 0.99 * batch_av_train_av_loss + 0.01 * batch_av_train_loss\n",
    "            normed_batch_centered_train_loss = ((elemwise_train_loss - batch_av_train_av_loss) / \n",
    "                                                (train_batch_size * chunk_length))\n",
    "            seqwise_train_loss = torch.sum(normed_batch_centered_train_loss, dim=1)\n",
    "            seqwise_cum_train_loss = torch.cumsum(normed_batch_centered_train_loss, dim=1)\n",
    "            for t, dec_feed in enumerate(model.dec_feeds):\n",
    "                dec_feed.reinforce(\n",
    "                    (-1) * (seqwise_train_loss - seqwise_cum_train_loss[:, t]).view(train_batch_size, 1)\n",
    "                )\n",
    "        elif reinforce_strategy == 'sequence':\n",
    "            rev_chunk_batch_torch_one_hot = torch.zeros(train_batch_size, chunk_length, vocab_size).cuda()\n",
    "            rev_chunk_batch_torch_one_hot.scatter_(\n",
    "                2, rev_chunk_batch_torch.data.view(train_batch_size, chunk_length, 1), 1\n",
    "            )\n",
    "            elemwise_train_loss = (-1) * F.log_softmax(\n",
    "                unscaled_logits.data.view(-1, vocab_size)\n",
    "            ).data.view(train_batch_size, chunk_length, vocab_size)[rev_chunk_batch_torch_one_hot.byte()].view(\n",
    "                train_batch_size, chunk_length\n",
    "            )\n",
    "            if step == 0:\n",
    "                train_av_loss = train_loss.data\n",
    "            else:\n",
    "                train_av_loss = 0.99 * train_av_loss + 0.01 * train_loss.data\n",
    "            seq_av_train_loss = torch.mean(elemwise_train_loss, dim=1, keepdim=True)\n",
    "            for t, dec_feed in enumerate(model.dec_feeds):\n",
    "                dec_feed.reinforce(\n",
    "                    (-1) * (seq_av_train_loss - train_av_loss) / (train_batch_size * chunk_length)\n",
    "                )\n",
    "        elif reinforce_strategy == 'none':\n",
    "            for t, dec_feed in enumerate(model.dec_feeds):\n",
    "                dec_feed.reinforce(torch.zeros(train_batch_size, 1).cuda())\n",
    "        else:\n",
    "            raise ValueError(\"Invalid reinforce_strategy: '{}'\".format(reinforce_strategy))\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        #nn.utils.clip_grad_norm(model.parameters(), max_norm=5)\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Eval:\n",
    "        chunk_batch = next(eval_batch_gen)\n",
    "        rev_chunk_batch = revert_words(chunk_batch)\n",
    "    \n",
    "        chunk_batch_torch = autograd.Variable(torch.from_numpy(chunk_batch).cuda(), requires_grad=False)\n",
    "        rev_chunk_batch_torch = autograd.Variable(torch.from_numpy(rev_chunk_batch).cuda(), requires_grad=False)\n",
    "    \n",
    "        unscaled_logits, outputs = model(chunk_batch_torch,\n",
    "                                         output_mode='argmax', feed_mode='sampling')\n",
    "        eval_loss = loss_function(unscaled_logits.view(-1, vocab_size), rev_chunk_batch_torch.view(-1))\n",
    "        eval_acc = torch.mean(torch.eq(outputs, rev_chunk_batch_torch).float())\n",
    "\n",
    "        train_losses[run].append(train_loss.data.cpu().numpy().mean())\n",
    "        train_accs[run].append(train_acc.data.cpu().numpy().mean())\n",
    "        eval_losses[run].append(eval_loss.data.cpu().numpy().mean())\n",
    "        eval_accs[run].append(eval_acc.data.cpu().numpy().mean())\n",
    "        \n",
    "        cum_train_loss += train_losses[run][-1]\n",
    "        cum_train_acc += train_accs[run][-1]\n",
    "        cum_eval_loss += eval_losses[run][-1]\n",
    "        cum_eval_acc += eval_accs[run][-1]\n",
    "    \n",
    "        if (step + 1) % print_skip == 0:\n",
    "            print('Step', step + 1)\n",
    "            print('Train loss: {:.2f}; train accuracy: {:.2f}'.format(\n",
    "                cum_train_loss / print_skip, cum_train_acc / print_skip\n",
    "            ))\n",
    "            print('Eval loss: {:.2f}; eval accuracy: {:.2f}'.format(\n",
    "                cum_eval_loss / print_skip, cum_eval_acc / print_skip\n",
    "            ))\n",
    "            cum_train_loss = 0\n",
    "            cum_eval_loss = 0\n",
    "            cum_train_acc = 0\n",
    "            cum_eval_acc = 0\n",
    "        \n",
    "            outputs_np = outputs.data.cpu().numpy()\n",
    "            for i in range(decode_batch_size):\n",
    "                print('{}|  vs  |{}'.format(\n",
    "                    ''.join(list(map(id_to_char, chunk_batch[i]))),\n",
    "                    ''.join(list(map(id_to_char, outputs_np[i])))\n",
    "                ))\n",
    "            \n",
    "            print('{:.2f}s from last print'.format(time() - last_print_time))\n",
    "            last_print_time = time()\n",
    "            print()\n",
    "    \n",
    "    print('{} steps took {:.2f}s\\n'.format(num_steps, time() - global_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def running_average(a, alpha=0.99):\n",
    "    av = a[:1]\n",
    "    for el in a[1:]:\n",
    "        av.append((1 - alpha) * el + alpha * av[-1])\n",
    "    return av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses_av = []\n",
    "train_accs_av = []\n",
    "\n",
    "eval_losses_av = []\n",
    "eval_accs_av = []\n",
    "\n",
    "for run in range(len(train_losses)):\n",
    "    train_losses_av.append(running_average(train_losses[run]))\n",
    "    train_accs_av.append(running_average(train_accs[run]))\n",
    "\n",
    "    eval_losses_av.append(running_average(eval_losses[run]))\n",
    "    eval_accs_av.append(running_average(eval_accs[run]))\n",
    "            \n",
    "train_losses_av_mean[reinforce_strategy] = np.mean(train_losses_av, axis=0)\n",
    "train_accs_av_mean[reinforce_strategy] = np.mean(train_accs_av, axis=0)\n",
    "\n",
    "eval_losses_av_mean[reinforce_strategy] = np.mean(eval_losses_av, axis=0)\n",
    "eval_accs_av_mean[reinforce_strategy] = np.mean(eval_accs_av, axis=0)\n",
    "    \n",
    "train_losses_av_std[reinforce_strategy] = np.std(train_losses_av, axis=0)\n",
    "train_accs_av_std[reinforce_strategy] = np.std(train_accs_av, axis=0)\n",
    "\n",
    "eval_losses_av_std[reinforce_strategy] = np.std(eval_losses_av, axis=0)\n",
    "eval_accs_av_std[reinforce_strategy] = np.std(eval_accs_av, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss; mean: 0.03, std: 0.00\n",
      "Train accuracy; mean: 0.99, std: 0.00\n",
      "Eval loss; mean: 0.04, std: 0.00\n",
      "Eval accuracy; mean: 0.99, std: 0.00\n"
     ]
    }
   ],
   "source": [
    "print('Train loss; mean: {:.2f}, std: {:.2f}'.format(\n",
    "    train_losses_av_mean[reinforce_strategy][-1], \n",
    "    train_losses_av_std[reinforce_strategy][-1]\n",
    "))\n",
    "print('Train accuracy; mean: {:.2f}, std: {:.2f}'.format(\n",
    "    train_accs_av_mean[reinforce_strategy][-1], \n",
    "    train_accs_av_std[reinforce_strategy][-1]\n",
    "))\n",
    "print('Eval loss; mean: {:.2f}, std: {:.2f}'.format(\n",
    "    eval_losses_av_mean[reinforce_strategy][-1], \n",
    "    eval_losses_av_std[reinforce_strategy][-1]\n",
    "))\n",
    "print('Eval accuracy; mean: {:.2f}, std: {:.2f}'.format(\n",
    "    eval_accs_av_mean[reinforce_strategy][-1], \n",
    "    eval_accs_av_std[reinforce_strategy][-1]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fec98d99da0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nalIZedXJNMEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fec98c9c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "strategies = ['none', 'sequence', 'element']\n",
    "for strategy in strategies:\n",
    "    plt.plot(range(num_steps), train_losses_av_mean[strategy])\n",
    "    plt.fill_between(\n",
    "        range(num_steps),\n",
    "        train_losses_av_mean[strategy] - train_losses_av_std[strategy],\n",
    "        train_losses_av_mean[strategy] + train_losses_av_std[strategy],\n",
    "        alpha=0.5\n",
    "    )\n",
    "plt.legend(strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chunk_length = 8, num_steps = 10000:\n",
    "# none: 0.03 +- 0.00\n",
    "# sequence: 0.03 +- 0.00\n",
    "# element: 0.12 +- 0.00\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
